import csv
import json
import logging
import asyncio
import aiohttp
import ssl
import time
import os
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'update_resources_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)

# Configuration from environment variables
API_V3_URL = os.getenv('API_V3_URL')
CLIENT_ID = os.getenv('CLIENT_ID')
SID = os.getenv('SID')

# Validate required environment variables
if not all([API_V3_URL, CLIENT_ID, SID]):
    logging.error("Missing required environment variables. Please check your .env file.")
    logging.error("Required variables: API_V3_URL, CLIENT_ID, SID")
    exit(1)

# Rate limiting configuration
RATE_LIMIT = 600  # requests per minute
BATCH_SIZE = 10   # concurrent requests
RATE_LIMIT_PER_SECOND = RATE_LIMIT / 60

# Headers for the request
headers = {
    'Content-Type': 'application/json',
    'sid': SID
}

# Payload for the PUT request
payload = {
    "data": {
        "tags": [],
        "amaTags": {
            "objects": {
                "autoGen": False,
                "autoGeneratedTags": [],
            },
            "keywords": {
                "autoGen": False,
                "autoGeneratedTags": []
            },
            "landmarks": {
                "autoGeneratedTags": [],
                "autoGen": False,
            },
            "faces": {
                "autoGeneratedTags": [],
                "autoGen": False
            }
        }
    }
}

class RateLimiter:
    def __init__(self, rate_limit_per_second):
        self.rate_limit = rate_limit_per_second
        self.tokens = rate_limit_per_second
        self.last_update = time.time()
        self.lock = asyncio.Lock()

    async def acquire(self):
        async with self.lock:
            now = time.time()
            time_passed = now - self.last_update
            self.tokens = min(self.rate_limit, self.tokens + time_passed * self.rate_limit)
            self.last_update = now

            if self.tokens < 1:
                sleep_time = (1 - self.tokens) / self.rate_limit
                await asyncio.sleep(sleep_time)
                self.tokens = 0
            else:
                self.tokens -= 1

class ResourceUpdater:
    def __init__(self):
        self.processed_file = Path('processed_ids.txt')
        self.errored_file = Path('errored_ids.txt')
        self.processed_ids = self._load_processed_ids()
        self.errored_ids = self._load_errored_ids()
        self.rate_limiter = RateLimiter(RATE_LIMIT_PER_SECOND)
        self.success_count = 0
        self.error_count = 0
        self.session = None
        self.start_time = None

    def _load_processed_ids(self):
        if self.processed_file.exists():
            return set(self.processed_file.read_text().splitlines())
        return set()

    def _load_errored_ids(self):
        if self.errored_file.exists():
            return set(self.errored_file.read_text().splitlines())
        return set()

    def _save_processed_id(self, resource_id):
        with self.processed_file.open('a') as f:
            f.write(f"{resource_id}\n")

    def _save_errored_id(self, resource_id):
        with self.errored_file.open('a') as f:
            f.write(f"{resource_id}\n")

    def _log_progress(self, total_processed):
        if self.start_time is None:
            self.start_time = time.time()
            return

        elapsed_time = time.time() - self.start_time
        requests_per_second = total_processed / elapsed_time if elapsed_time > 0 else 0
        logging.info(f"Progress: {total_processed} processed, "
                    f"Rate: {requests_per_second:.2f} req/s, "
                    f"Success: {self.success_count}, "
                    f"Errors: {self.error_count}")

    async def update_resource(self, resource_id):
        """Make PUT request to update a single resource."""
        if resource_id in self.processed_ids:
            logging.info(f"Skipping already processed Resource ID: {resource_id}")
            return None

        if resource_id in self.errored_ids:
            logging.info(f"Skipping previously errored Resource ID: {resource_id}")
            return None

        await self.rate_limiter.acquire()
        url = f"{API_V3_URL}/api/3.0.0/{CLIENT_ID}/resource/{resource_id}?verbose=null"
        
        try:
            async with self.session.put(url, headers=headers, json=payload, ssl=False) as response:
                response_text = await response.text()
                if response.status == 200:
                    logging.info(f"Success - Resource ID: {resource_id} - Status: {response.status}")
                    self.success_count += 1
                    self._save_processed_id(resource_id)
                    return True
                else:
                    error_msg = f"Status Code: {response.status} - Response: {response_text}"
                    logging.error(f"Error - Resource ID: {resource_id} - {error_msg}")
                    self.error_count += 1
                    self._save_errored_id(resource_id)
                    return False
        except Exception as e:
            error_msg = f"Exception: {str(e)}"
            logging.error(f"Error - Resource ID: {resource_id} - {error_msg}")
            self.error_count += 1
            self._save_errored_id(resource_id)
            return False

    async def process_batch(self, batch):
        """Process a batch of resource IDs concurrently."""
        tasks = [self.update_resource(resource_id) for resource_id in batch]
        await asyncio.gather(*tasks)
        self._log_progress(self.success_count + self.error_count)

    async def run(self):
        """Main function to process the CSV file and update resources."""
        logging.info("Starting resource update process")
        if self.processed_ids:
            logging.info(f"Resuming from previous run. Already processed: {len(self.processed_ids)} resources")
        if self.errored_ids:
            logging.info(f"Found {len(self.errored_ids)} previously errored resources (will be skipped)")
        
        try:
            # Create SSL context that doesn't verify certificates
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            
            conn = aiohttp.TCPConnector(ssl=False)
            timeout = aiohttp.ClientTimeout(total=30)  # 30 seconds timeout
            
            async with aiohttp.ClientSession(connector=conn, timeout=timeout) as session:
                self.session = session
                batch = []
                
                with open('list.csv', 'r') as file:
                    csv_reader = csv.reader(file)
                    for row in csv_reader:
                        if row:  # Skip empty rows
                            resource_id = row[0].strip()
                            batch.append(resource_id)
                            
                            if len(batch) >= BATCH_SIZE:
                                await self.process_batch(batch)
                                batch = []
                    
                    # Process remaining resources
                    if batch:
                        await self.process_batch(batch)
        
        except FileNotFoundError:
            logging.error("list.csv file not found")
            return
        except Exception as e:
            logging.error(f"Unexpected error: {str(e)}")
            return
        finally:
            elapsed_time = time.time() - self.start_time if self.start_time else 0
            logging.info(
                f"Process completed - Successful updates: {self.success_count}, "
                f"Errors: {self.error_count}, "
                f"Total time: {elapsed_time:.2f} seconds, "
                f"Average rate: {(self.success_count + self.error_count) / elapsed_time:.2f} req/s"
            )

async def main():
    updater = ResourceUpdater()
    await updater.run()

if __name__ == "__main__":
    asyncio.run(main())